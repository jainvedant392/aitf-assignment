import { useState, useRef, useCallback, useEffect } from "react";

export function useDeepgramVoiceRecording() {
  const [isRecording, setIsRecording] = useState(false);
  const [isProcessing, setIsProcessing] = useState(false);
  const [transcript, setTranscript] = useState("");
  const [confidence, setConfidence] = useState(0);
  const [error, setError] = useState(null);
  const [isSupported, setIsSupported] = useState(true);

  const mediaRecorderRef = useRef(null);
  const audioChunksRef = useRef([]);
  const streamRef = useRef(null);

  // Check browser support on mount
  useEffect(() => {
    const checkSupport = () => {
      if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
        setIsSupported(false);
        setError("ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯ãƒžã‚¤ã‚¯éŒ²éŸ³ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“");
        return;
      }

      if (
        !MediaRecorder.isTypeSupported("audio/webm") &&
        !MediaRecorder.isTypeSupported("audio/wav")
      ) {
        setIsSupported(false);
        setError("ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã¯å¯¾å¿œã™ã‚‹éŸ³å£°å½¢å¼ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“");
        return;
      }

      console.log("âœ… Deepgram voice recording supported");
    };

    checkSupport();
  }, []);

  const startRecording = useCallback(async () => {
    if (!isSupported || isRecording || isProcessing) {
      return;
    }

    try {
      setError(null);
      setTranscript("");
      setConfidence(0);

      console.log("ðŸŽ¤ Requesting microphone access...");

      // Request microphone with optimized settings for speech
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000, // Optimal for speech recognition
          channelCount: 1, // Mono audio
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
        },
      });

      streamRef.current = stream;

      // Determine the best audio format
      let mimeType = "audio/webm";
      if (MediaRecorder.isTypeSupported("audio/webm;codecs=opus")) {
        mimeType = "audio/webm;codecs=opus";
      } else if (MediaRecorder.isTypeSupported("audio/wav")) {
        mimeType = "audio/wav";
      }

      // Create MediaRecorder
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType,
        audioBitsPerSecond: 64000, // Good quality for speech
      });

      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
          console.log("ðŸ“Š Audio chunk recorded:", event.data.size, "bytes");
        }
      };

      mediaRecorder.onstop = async () => {
        console.log("ðŸ›‘ Recording stopped, processing...");
        setIsRecording(false);
        setIsProcessing(true);

        // Stop all audio tracks
        if (streamRef.current) {
          streamRef.current.getTracks().forEach((track) => track.stop());
          streamRef.current = null;
        }

        // Create audio blob
        const audioBlob = new Blob(audioChunksRef.current, {
          type: mimeType,
        });

        console.log(
          "ðŸŽµ Audio blob created:",
          audioBlob.size,
          "bytes",
          audioBlob.type
        );

        if (audioBlob.size === 0) {
          setError("éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„");
          setIsProcessing(false);
          return;
        }

        // Send to backend for transcription
        await processAudioWithBackend(audioBlob);
      };

      mediaRecorder.onerror = (event) => {
        console.error("âŒ MediaRecorder error:", event.error);
        setError(`éŒ²éŸ³ã‚¨ãƒ©ãƒ¼: ${event.error?.message || "ä¸æ˜Žãªã‚¨ãƒ©ãƒ¼"}`);
        setIsRecording(false);
        setIsProcessing(false);
      };

      // Start recording with data collection every 100ms for responsiveness
      mediaRecorder.start(100);
      setIsRecording(true);
      console.log("âœ… Recording started with", mimeType);
    } catch (error) {
      console.error("âŒ Failed to start recording:", error);

      let errorMessage = "éŒ²éŸ³é–‹å§‹ã«å¤±æ•—ã—ã¾ã—ãŸ";
      if (error.name === "NotAllowedError") {
        errorMessage =
          "ãƒžã‚¤ã‚¯ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒæ‹’å¦ã•ã‚Œã¾ã—ãŸã€‚ãƒ–ãƒ©ã‚¦ã‚¶ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„";
      } else if (error.name === "NotFoundError") {
        errorMessage =
          "ãƒžã‚¤ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒžã‚¤ã‚¯ãŒæŽ¥ç¶šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„";
      } else if (error.name === "NotSupportedError") {
        errorMessage = "ã“ã®ãƒ–ãƒ©ã‚¦ã‚¶ã§ã¯éŒ²éŸ³ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“";
      }

      setError(errorMessage);
      setIsRecording(false);
    }
  }, [isSupported, isRecording, isProcessing]);

  const stopRecording = useCallback(() => {
    if (!isRecording || !mediaRecorderRef.current) {
      return;
    }

    try {
      console.log("â¹ï¸ Stopping recording...");
      mediaRecorderRef.current.stop();
    } catch (error) {
      console.error("âŒ Error stopping recording:", error);
      setError(`éŒ²éŸ³åœæ­¢ã‚¨ãƒ©ãƒ¼: ${error.message}`);
      setIsRecording(false);
      setIsProcessing(false);
    }
  }, [isRecording]);

  const cancelRecording = useCallback(() => {
    console.log("âŒ Cancelling recording...");

    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
    }

    // Stop audio tracks immediately
    if (streamRef.current) {
      streamRef.current.getTracks().forEach((track) => track.stop());
      streamRef.current = null;
    }

    setIsRecording(false);
    setIsProcessing(false);
    setTranscript("");
    setConfidence(0);
    setError(null);
    audioChunksRef.current = [];
  }, [isRecording]);

  const processAudioWithBackend = async (audioBlob) => {
    try {
      console.log("ðŸš€ Sending audio to backend for processing...");

      // Create FormData for file upload
      const formData = new FormData();
      formData.append("audio", audioBlob, "recording.webm");
      formData.append("language", "ja"); // Japanese

      // Send to backend voice processing endpoint
      const response = await fetch(
        "http://localhost:5000/api/chat/voice/process",
        {
          method: "POST",
          body: formData,
        }
      );

      if (!response.ok) {
        throw new Error(`HTTP error! status: ${response.status}`);
      }

      const result = await response.json();
      console.log("âœ… Backend processing result:", result);

      if (result.success) {
        const transcriptionData = result.transcription;

        setTranscript(transcriptionData.transcript);
        setConfidence(transcriptionData.confidence);

        console.log(
          `ðŸŽ¯ Transcription: "${transcriptionData.transcript}" (confidence: ${transcriptionData.confidence})`
        );

        // The full result includes chat response, but we'll let the parent component handle that
        // Emit the full result for the chat interface to process
        if (window.deepgramResult) {
          window.deepgramResult(result);
        }
      } else {
        throw new Error(result.error || "éŸ³å£°å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ");
      }
    } catch (error) {
      console.error("âŒ Audio processing failed:", error);
      setError(`éŸ³å£°å‡¦ç†ã‚¨ãƒ©ãƒ¼: ${error.message}`);
    } finally {
      setIsProcessing(false);
    }
  };

  const resetTranscript = useCallback(() => {
    setTranscript("");
    setConfidence(0);
    setError(null);
  }, []);

  // For compatibility with existing useVoiceRecognition interface
  const isListening = isRecording;

  return {
    // Core recording states
    isRecording,
    isProcessing,
    transcript,
    confidence,
    error,
    isSupported,

    // Control functions
    startRecording,
    stopRecording,
    cancelRecording,
    resetTranscript,

    // Compatibility with existing interface
    isListening,
    startListening: startRecording,
    stopListening: stopRecording,
  };
}
