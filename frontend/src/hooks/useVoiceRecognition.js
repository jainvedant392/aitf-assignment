import { useState, useEffect, useRef, useCallback } from 'react'

export function useVoiceRecognition() {
  const [isListening, setIsListening] = useState(false)
  const [transcript, setTranscript] = useState('')
  const [isSupported, setIsSupported] = useState(true)
  const [error, setError] = useState(null)
  
  const recognitionRef = useRef(null)
  const timeoutRef = useRef(null)

  // Initialize speech recognition
  useEffect(() => {
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition
    
    if (!SpeechRecognition) {
      setIsSupported(false)
      return
    }

    const recognition = new SpeechRecognition()
    recognition.continuous = true
    recognition.interimResults = true
    recognition.lang = 'ja-JP' // Japanese language
    recognition.maxAlternatives = 1

    recognition.onstart = () => {
      setIsListening(true)
      setError(null)
      console.log('ðŸŽ¤ Voice recognition started')
    }

    recognition.onresult = (event) => {
      let finalTranscript = ''
      let interimTranscript = ''

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcriptPart = event.results[i][0].transcript
        
        if (event.results[i].isFinal) {
          finalTranscript += transcriptPart
        } else {
          interimTranscript += transcriptPart
        }
      }

      setTranscript(interimTranscript || finalTranscript)

      // Auto-stop after silence
      if (timeoutRef.current) {
        clearTimeout(timeoutRef.current)
      }
      
      timeoutRef.current = setTimeout(() => {
        if (finalTranscript.trim()) {
          recognition.stop()
        }
      }, 2000)
    }

    recognition.onerror = (event) => {
      console.error('ðŸš« Voice recognition error:', event.error)
      setIsListening(false)
      
      let errorMessage = 'éŸ³å£°èªè­˜ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ'
      switch (event.error) {
        case 'no-speech':
          errorMessage = 'éŸ³å£°ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ'
          break
        case 'audio-capture':
          errorMessage = 'ãƒžã‚¤ã‚¯ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“'
          break
        case 'not-allowed':
          errorMessage = 'ãƒžã‚¤ã‚¯ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒè¨±å¯ã•ã‚Œã¦ã„ã¾ã›ã‚“'
          break
        case 'network':
          errorMessage = 'ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¨ãƒ©ãƒ¼ã§ã™'
          break
      }
      setError(errorMessage)
    }

    recognition.onend = () => {
      setIsListening(false)
      if (timeoutRef.current) {
        clearTimeout(timeoutRef.current)
      }
      console.log('ðŸ›‘ Voice recognition ended')
    }

    recognitionRef.current = recognition

    return () => {
      if (timeoutRef.current) {
        clearTimeout(timeoutRef.current)
      }
      if (recognition) {
        recognition.stop()
      }
    }
  }, [])

  const startListening = useCallback(() => {
    if (!isSupported || !recognitionRef.current) {
      setError('éŸ³å£°èªè­˜ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã›ã‚“')
      return
    }

    try {
      setTranscript('')
      setError(null)
      recognitionRef.current.start()
    } catch (error) {
      console.error('Failed to start recognition:', error)
      setError('éŸ³å£°èªè­˜ã‚’é–‹å§‹ã§ãã¾ã›ã‚“ã§ã—ãŸ')
    }
  }, [isSupported])

  const stopListening = useCallback(() => {
    if (recognitionRef.current && isListening) {
      recognitionRef.current.stop()
    }
  }, [isListening])

  const resetTranscript = useCallback(() => {
    setTranscript('')
    setError(null)
  }, [])

  return {
    isListening,
    transcript,
    isSupported,
    error,
    startListening,
    stopListening,
    resetTranscript
  }
}
